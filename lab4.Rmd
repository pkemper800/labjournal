---
title: "lab4"
author: "Paige Kemper"
date: "2025-09-16"
output: html_document
---


```{r, globalsettings, echo=FALSE, warning=FALSE, results='hide'}
library(knitr)

knitr::opts_chunk$set(echo = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=100),tidy=TRUE, warning = FALSE, message = FALSE,comment = "#>", cache=TRUE, class.source=c("test"), class.output=c("test2"))
options(width = 100)
rgl::setupKnitr()


colorize <- function(x, color) {sprintf("<span style='color: %s;'>%s</span>", color, x) }


```

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
#klippy::klippy(color = 'darkred')
#klippy::klippy(tooltip_message = 'Click to copy', tooltip_success = 'Done')
```

Last compiled on `r format(Sys.time(), '%B, %Y')`

<br>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# DURING CLASS:

## sample using igraph

``` {r}
require(igraph)
g <- make_graph("Zachary")
plot(g)

# measures of centrality 
# degrees - is the most simple term for centrality is the degrees of connections 
# if want to describe network, what dimensions describe? if it is complete, the size of it, what kind of relationship (un/directed), if the tie is weighed (and the node type)
# So: degree centrality, then bridges -- so, if the story is that there are differences in degree centrality, could illustrate nodes by differing size
``` 

Note: 
*Measures of centrality:*
Degrees - is the most simple term for centrality is the degrees of connection. 

If want to describe network, what dimensions describe? 
-   If it is complete, the size of it, what kind of relationship (un/directed), if the tie is weighed (and the node type)

So: degree centrality, then bridges -- if the story is that there are differences in degree centrality, could illustrate nodes by differing size for example. 


## Local transitivity

``` {r}
gmat <- as_adjacency_matrix(g, type = "both", sparse = FALSE)
gmat

#THEN - LOOK AT DESCRIPTIVE STATISTICS
#SIZE
# number of nodes
vcount(g)
# number of edges
ecount(g) 

#DEGREE
igraph::degree(g)
# hist(table(degree(g)), xlab='indegree', main= 'Histogram of indegree') - every number is the degree level of each actor -- and see it is heavily skewed -- 

#TRANSITIVITY
# be aware that directed graphs are considered as undirected. but g is undirected.
igraph::transitivity(g, type = c("localundirected"), isolates = c("NaN", "zero")) #differences pop out less 

#BETWEENNES
# be aware that directed graphs are considered as undirected. but g is undirected.
igraph::transitivity(g, type = c("localundirected"), isolates = c("NaN", "zero"))


  # with this we see big differences in transivity and betweenness


``` 


## Global transitivity 

Next, moving from local to global transitivity - look at triads. Global transitivity below. 

Note: reviewing dyad - then triad - and since it is undirected, it is less difficult to calculate. 
Now, triad census vs triad allegation. Global: number of observed over possible - **can identify all transitive triads and all possible triads.** 

``` {r}
# Had to comment this out to get it to knit ? 

#igraph::dyad.census(g)

#igraph::triad.census(g)
# I will use sna because it shows the names of the triads as well.

#install.packages("sna")
#library(sna)

#sna::triad.census(gmat)
#unloadNamespace("sna")  #I will detach this package again, otherwise it will interfere with all kind of functions from igraph, and my students will hate me for that.

####


igraph::transitivity(g, type = "global")
sna::gtrans(gmat) #triad census a different way 

triad_g <- data.frame(sna::triad.census(gmat)) #save as df

transitivity_g <- (3 * triad_g$X300)/(triad_g$X201 + 3 * triad_g$X300) #X300 is variable for transitive triad (the fully closed triad)
# we multiply by 3 because there are 3 possible transitive triads
transitivity_g

``` 


## Network visualization: Letâ€™s make size proportional to betweeness score

``` {r}
# changing V
V(g)$size = betweenness(g, normalized = T, directed = FALSE) * 60 + 10  #after some trial and error
 ## multiplication - changing 60 changes the difference in size,, adding 10 makes the smallest visible
plot(g, mode = "undirected")


``` 


## Reduce overlap
Goal: I want to hold printing device constant, and then reduce overlap. To do this, the idea is to push least central egos out. 

``` {r}
# igraph, want no pverlap - igraph plotting no overlap -- a lot of layout functions -- want to hold printing device constant, and then reduce overlap...the idea is to push least central egos out 

set.seed(2345)
l <- layout_with_mds(g)  #https://igraph.org/r/doc/layout_with_mds.html
plot(g, layout = l)
# story in second plot: 3 clusters, around 1, around 34 - and in between (which wasn't as clear before)

``` 


### Can also do this manually
**can save coordinates of layout, and then tweak coordinates ourself - but that is no longer an objective layout function**

``` {r}
l  #let us take a look at the coordinates -- and now we save them
l[1, 1] <- 4
l[34, 1] <- -3.5 # the coordinates are the x and y, and this makes the 1 and the 34 more positive / more negative
plot(g, layout = l)

``` 

looking at this, maybe 1, 33 and 34 are instructures, and numbers closer to 1 are more similar to 1 and 34, 34. 


## Get final picture 

Last step, manipulate data more and get final picture

``` {r}
plot(g, layout = l, margin = c(0, 0, 0, 0))
legend(x = -2, y = -1.5, c("Note: the position of nodes 1 and 34 have been set by Jochem Tolsma \n for visualisation purposes only and do not reflect network properties"),
    bty = "n", cex = 0.8)
``` 


**Other notes** 

Notes: size ---> degree of centrality! 

Review of twitter research -- 

jochemtolsma.github.io/Twitter/figures.html

-    Looking at data of parliament from twitter -- some type of segmentation by party - coloring the different parties makes it much easier to discern differences. can look at directed / undirected (mutual).
-    Then used same plots, differed by algorithm - same colors (party) cluster together - prove to reviewer that algorithm outputs cluster. 
Take home message: clustering by party. PVV is outlier. proves that there is differing levels of segmentation, too. 


Remember: 
descriptive: macro level, ego, triad, etc. 
descriptive research question - 50-75% of final project assignment 



``` {r}
``` 


``` {r}
``` 


# After class: 
Codereview 2 



# BEFORE CLASS: 

## First Open Alex Attempt

### clean workspace
``` {r}
# start with clean workspace
rm(list = ls())
```

### Install packages 

``` {r}
#install.packages('data.table')
library(data.table)  # mainly for faster data handling

library(tidyverse)  # I assume you already installed this one!
#install.packages('httr') # we don't need this for now require(httr)
#install.packages("xml2")
require(xml2)
#install.packages("rvest")
require(rvest)
#install.packages("devtools")
require(devtools)

#RSelenium usefull for websites where the data is 'loaded in', rvest cant handle that
#packages: "RSelenium", "rvest", "tidyverse", "netstat", "pingr", "stringr"

#install.packages("RSelenium")
#install.packages("netstat")
#install.packages("pingr")
```


Had to comment this out to knit 

``` {r}
# install.packages("RSelenium") # 
#install.packages("rvest")
#install.packages("tidyverse")
# install.packages("netstat") # had to comment this out to knit
#install.packages("pingr") # had to comment this out to knit
#install.packages("stringr")# had to comment this out to knit
#install.packages("selenider")# had to comment this out to knit

library(selenider)
library(rvest)
library(tidyverse)
library(netstat)
library(pingr)
library(stringr)

```




```{r}
rm(list = ls())

fpackage.check <- function(packages) {
    lapply(packages, FUN = function(x) {
        if (!require(x, character.only = TRUE)) {
            install.packages(x, dependencies = TRUE)
            library(x, character.only = TRUE)
        }
    })
}

fsave <- function(x, file = NULL, location = "./data/processed/") {
    ifelse(!dir.exists("data"), dir.create("data"), FALSE)
    ifelse(!dir.exists("data/processed"), dir.create("data/processed"), FALSE)
    if (is.null(file))
        file = deparse(substitute(x))
    datename <- substr(gsub("[:-]", "", Sys.time()), 1, 8)
    totalname <- paste(location, datename, file, ".rda", sep = "")
    save(x, file = totalname)  #need to fix if file is reloaded as input name, not as x. 
}

fload <- function(filename) {
    load(filename)
    get(ls()[ls() != "filename"])
}

fshowdf <- function(x, ...) {
    knitr::kable(x, digits = 2, "html", ...) %>%
        kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
        kableExtra::scroll_box(width = "100%", height = "300px")
}

```


### Code for attmepting to scrape Open Alex (just radboud)

``` {r}
options(openalexR.mailto = "paige.kemper@ru.nl") 

library(openalexR)
 df_test <- oa_fetch(entity= "author", search = "Jochem+Tolsma", mailto = "paige.kemper@ru.nl")
 df_test
``` 

Had to comment out following code to knit
``` {R}
require(openalexR)
fshowdf(df_test) # had to comment this out to knit

# test finding radboud university institution in Open Alex 

df_ru <-oa_fetch(entity = "institution", search = "Radboud+University+Nijmegen", mailto="paige.kemper@ru.nl") #author.id = df_test$id)
df_ru

df_insts <- oa_fetch(entity = "institutions", search = "radboud", verbose = TRUE)$id[1]
df_insts

f_inst <- function(x) {
    oa_fetch(entity = "institutions", search = x)$id[1]
}
df_insts
```


### Code for attempting to search for specific author in Open Alex

``` {r}
## find authors with specific names
#start with column

authors <- c("Jochem Tolsma", "Tom van der Meer", "Maurice Gesthuizen", "Michael Savelkoul")

df_authors <- NA
for (i in 1:length(authors)) {
    df_authors[i] <- oa_fetch(entity = "authors", search = authors[i], affiliations.institution.id = df_insts)[1,
        ] %>%
        select(id)
}
df_authors <- unlist(df_authors)
df_authors #returns websites for each author


#Scrap code
#df_ru1 <- oa_fetch(entity = "Raboud Univeristy Nijmegen", search = "authors", mailto = "paige.kemper@ru.nl" )
#df_ru1 <- oa_fetch(entity = "Radboud University Nijmegen", author.id=df_ru1$id )
#df_papers$id[1] #website it is coming from...
#df_papers <- oa_fetch(entity = "works", author.id = df$id)
#df_papers$author[1]
```


### Searching institution in Open Alex - another attempt

``` {r}
#can search institutions within openalex
df_institution <- oa_fetch(entity = "institutions", search = "radboud university nijmegen")$id[1]
df_institution

# df_institution2 <- oa_fetch()

#can search authors
df_author <- oa_fetch(entity = "author", search = "Tolsma")


# searching radboud
ru_id <- institutions <- oa_fetch(entity = "institutions", search = "Radboud University") #get the id of the radboud university 
ru_id

# searching UvA
uva_id <- oa_fetch(entity = "institutions", search = "Universiteit van Amsterdam") #get uva's id 
uva_id
```


``` {r}


```



``` {r}
```

load("C:/Users/paigek/Downloads/liss_cdn.Rdata")

